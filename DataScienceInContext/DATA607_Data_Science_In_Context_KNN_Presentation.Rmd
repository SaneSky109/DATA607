---
title: "DATA 607: Data Science in Context - K Nearest Neighbors Algorithm"
author: "Eric Lehmphul"
date: "10/27/2021"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## KNN

- Supervised learning algorithm used in both **classification** and **regression** problems
  * In classification problems, new data points will be classified in a particular class
  * In regression problems, new data points will be labeled based on the average value of k determined nearest neighbors

- KNN uses the proximity between points to determine label data points
  * Euclidean
  * Manhattan
  * Minkowski
- Normalization of the data is important to eliminate bias
  * Min-Max Normalization
  * Z-Score Normalization

![Regession Example](C:\Users\ericl\OneDrive\Documents\CUNY MS in Data Science\DATA-607\Projects\Data Science in Context\knn_regression.PNG)
![Classification Example](C:\Users\ericl\OneDrive\Documents\CUNY MS in Data Science\DATA-607\Projects\Data Science in Context\knn_classification.PNG)


## Why use KNN

- It is simple and intuitive.

- Does not require any specific assumptions be made concerning the data and can be applied to any data distribution.


## Limitations of KNN

- Accuracy of model depends on the quality of the data.

- If using a large number of predictors and or number of records, calculating the distances between points will be computationally expensive.



## Euclidean Distance

- Popular distance measure used in KNN

$$
Distance = \sqrt{(x_1 - u_1)^2 + (x_2 - u_2)^2 + ...+(x_p - u_p)^2}
$$

```{r}
set.seed(105)
x <- matrix(runif(12),ncol=2)

G1 <- cccd::nng(x,k=1, method = "Euclidean")

plot(G1)
```


## Choosing k

- **k** is the number of nearby neighbors to be used to classify the new record
  * If k is too small it is sensitive to noise
  * If k is too large it may include may include majority points from other classes
- The square root of the training dataset is usually a good value for k, but not always the optimal k.

```{r}
set.seed(104)
x <- matrix(runif(12),ncol=2)

par(mfrow=c(1,2))

G1 <- cccd::nng(x,k=1, method = "Euclidean")
G2 <- cccd::nng(x,k=3, method = "Euclidean")

plot(G1, main = "k = 1")
plot(G2, main = "k = 3")
```


## Demo: Classifying Fruit

* Load Libraries
* Read Dataset
* Normalize Data
* Create Training and Testing Dataframes
* Create Model
* Model Results


## Load Libraries

```{r, echo=TRUE,warning=FALSE}
library(class) # for knn model
library(caret) # for confusion matrix
library(ggplot2) # for graphs
library(gghighlight) # also for graphs
```


## Dataset

* The data contains descriptive attributes related to a fruit's diameter, weight, and RGB color intensity.
* Target Variable: 
  - Fruit (Grapefruit or Orange)
* Predictor Variables: 
  - Diameter of Fruit
  - Weight of Fruit
  - Red color intensity
  - Green color intensity
  - Blue Color Intensity
* The dataset was acquired from Kaggle.com (https://www.kaggle.com/joshmcadams/oranges-vs-grapefruit)

```{r,echo=TRUE,warning=FALSE}
fruit.data <- read.csv("https://raw.githubusercontent.com/SaneSky109/DATA607/main/DataScienceInContext/Data/citrus.csv")

fruit.data$name <- factor(fruit.data$name)

head(fruit.data)
```


## Normalize Data

* Since distance is the main measurement in knn, normalization is extremely important to eliminate bias.
* I scaled my data using Min-Max normalization.
  - $\frac{value-min}{max-min}$

```{r, echo=TRUE}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}
```

```{r,echo=TRUE}
norm.fruit <- as.data.frame(lapply(fruit.data[,-1], normalize)) 

norm.fruit$name <- fruit.data$name

head(norm.fruit)
```


## Create Training and Testing Data Frames

```{r,echo=TRUE}
set.seed(105)

data.sample <- sample(1:nrow(norm.fruit), size = nrow(norm.fruit)*0.7, replace = FALSE) # random sample of 70% of data

train.fruit <- norm.fruit[data.sample,]
test.fruit <- norm.fruit[-data.sample,]
```


## Create Model

* The **train** argument takes the training dataset (not including target variable)
* The **test** argument takes the testing dataset (not including target variable)
* The **cl** argument takes the vector of true classifications of the training dataset
* The **k** argument takes number of nearest neighbors considered 

```{r,echo=TRUE}
model <- knn(train = train.fruit[,1:5], test = test.fruit[,1:5], cl = train.fruit[,6], k = sqrt(nrow(train.fruit)))
```


## Results

```{r,echo=TRUE}
actual <- test.fruit[,6]

results <- table(model, actual)
results
```

```{r,echo=TRUE}
confusionMatrix(results)
```


## Find Model with optimal k in terms of Accuracy

```{r,echo=TRUE}
k.optm <- 1

for (i in 1:sqrt(nrow(train.fruit))) {
  model <- knn(train = train.fruit[,1:5], test = test.fruit[,1:5], cl = train.fruit[,6], k = i)
  k.optm[i] <- 100*sum(test.fruit[,6]==model)/nrow(test.fruit)
  k <- i
  cat(k,'=', k.optm[i],'\n')
}
```


```{r,echo=TRUE}
accuracy<-as.data.frame(cbind(k = c(1:83),k.optm))
ggplot(accuracy, aes(x=k,y=k.optm)) +
  geom_point() +
  gghighlight(k.optm == max(k.optm)) +
  geom_label(aes(label = k),
              hjust = 1.2, vjust = 1.2, fill = "dark blue", colour = "white", alpha= 0.5)
```


## Q/A

Thank you for listening. Are there any questions?

